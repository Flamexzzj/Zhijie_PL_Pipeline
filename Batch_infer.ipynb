{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# This is MAC branch\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from scipy.special import softmax\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PL_Support_Codes.models import build_model\n",
    "from PL_Support_Codes.tools import load_cfg_file\n",
    "from PL_Support_Codes.datasets.utils import generate_image_slice_object\n",
    "from PL_Support_Codes.utils.utils_image import ImageStitcher_v2 as ImageStitcher\n",
    "from PL_Support_Codes.datasets import build_dataset, tensors_and_lists_collate_fn\n",
    "\n",
    "from PL_Support_Codes.models.lf_model import LateFusionModel\n",
    "from PL_Support_Codes.models.ef_model import EarlyFusionModel\n",
    "from PL_Support_Codes.models.water_seg_model import WaterSegmentationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Setup model parameters\n",
    "dataset_name = \"batch_infer\"\n",
    "infer_split = \"all\"\n",
    "infer_seed_num = 0\n",
    "infer_train_split_pct = 0.0\n",
    "infer_num_workers = 0\n",
    "n_classes_model = 3\n",
    "model_used_here = \"unet_cbam\"\n",
    "# optimizer_used = \"adam\"\n",
    "\n",
    "base_save_dir = \"/Users/zhijiezhang/Current_Projects/Zhijie_PL_Pipeline/Infered_result/trif1\"\n",
    "checkpoint_path = \"/Users/zhijiezhang/Current_Projects/Zhijie_PL_Pipeline/Trained_model/CBAM/checkpoints/THP_CBAM_HPC.ckpt\"\n",
    "\n",
    "# Root folder containing the directories you waant to run inference on, under this folder, there should be different dates folder, within the dates folder, there should be imgs\n",
    "ROOT_FOLDER = \"/Users/zhijiezhang/Current_Projects/Zhijie_PL_Pipeline/DATA/trif_example/\"\n",
    "\n",
    "# JSON file path\n",
    "JSON_FILE = \"/Users/zhijiezhang/Current_Projects/Zhijie_PL_Pipeline/Zhijie_PL_Pipeline/dataset_dirs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def infer():\n",
    "    # Load configuration file.\n",
    "    experiment_dir = '/'.join(checkpoint_path.split('/')[:-2])\n",
    "    cfg_path = os.path.join(experiment_dir, 'config.yaml')\n",
    "    print(\"check point file path: \", checkpoint_path)\n",
    "    cfg = load_cfg_file(cfg_path)\n",
    "\n",
    "    if 'model_n_classes' in cfg:\n",
    "        n_classes_used = cfg.model_n_classes\n",
    "    else:\n",
    "        n_classes_used = n_classes_model\n",
    "    \n",
    "    if 'model_used' in cfg:\n",
    "        model_used_infer = cfg.model_used\n",
    "    else:\n",
    "        model_used_infer = model_used_here\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if not os.path.exists(base_save_dir):\n",
    "        os.makedirs(base_save_dir)\n",
    "    print(\"Saving inference to: \",base_save_dir)\n",
    "    # Load dataset.\n",
    "    slice_params = generate_image_slice_object(cfg.crop_height, cfg.crop_width, min(cfg.crop_height, cfg.crop_width))\n",
    "    eval_dataset = build_dataset(dataset_name,\n",
    "                                 infer_split,\n",
    "                                 slice_params,\n",
    "                                 sensor=cfg.dataset.sensor,\n",
    "                                 channels=cfg.dataset.channels,\n",
    "                                 n_classes=n_classes_used,\n",
    "                                 norm_mode=cfg.norm_mode,\n",
    "                                 eval_region=cfg.eval_region,\n",
    "                                 ignore_index=cfg.ignore_index,\n",
    "                                 seed_num=infer_seed_num,\n",
    "                                 train_split_pct=infer_train_split_pct,\n",
    "                                 output_metadata=True,\n",
    "                                 # ** allows us to pass in any additional arguments to the dataset as dictionary.\n",
    "                                 **cfg.dataset.dataset_kwargs)\n",
    "\n",
    "    eval_loader = DataLoader(eval_dataset,\n",
    "                             batch_size=cfg.batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=infer_num_workers, collate_fn=tensors_and_lists_collate_fn)\n",
    "    \n",
    "    MODELS = {\n",
    "        'ms_model': WaterSegmentationModel,\n",
    "        'ef_model': EarlyFusionModel,\n",
    "        'lf_model': LateFusionModel\n",
    "    }\n",
    "# here need to retrain model and save new config file, in the new one it\n",
    "#should be   cfg.model_used\n",
    "    model = MODELS[cfg.model.name].load_from_checkpoint(checkpoint_path,\n",
    "                                       in_channels=eval_dataset.n_channels,\n",
    "                                       n_classes=eval_dataset.n_classes,\n",
    "                                       lr=cfg.lr,\n",
    "                                       log_image_iter=cfg.log_image_iter,\n",
    "                                       to_rgb_fcn=eval_dataset.to_RGB,\n",
    "                                       ignore_index=eval_dataset.ignore_index,\n",
    "                                       model_used=model_used_infer,\n",
    "                                       **cfg.model.model_kwargs)\n",
    "    model._set_model_to_eval()\n",
    "\n",
    "    # Get device.\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        print(\"!!!!!! CUDA is available!!!!!!\")\n",
    "    else:\n",
    "        device = 'mps'\n",
    "        print(\"!!!!!! CUDA is not available, using MPS !!!!!!\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate predictions on target dataset.\n",
    "    pred_canvases = {}\n",
    "    with torch.no_grad(): #no_grad() prevents gradiant calculation, which is not needed for inference.\n",
    "        # breakpoint()\n",
    "        for batch in tqdm(eval_loader, colour='green', desc='Generating predictions'):\n",
    "            # Move batch to device.\n",
    "            for key, value in batch.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    batch[key] = value.to(dtype=torch.float32).to(device)\n",
    "\n",
    "            # Generate predictions.\n",
    "            # this pass the current batch into model, to generate prediction, \n",
    "            #at this stage, the output is the raw output that is the probability distribution over the classes for the corresponding pixel in the input image. This distribution can be interpreted as \n",
    "            #the model's confidence in each class for that pixel. They are the raw score of what model thinks the possibility of each class for each pixel.\n",
    "            output = model(batch).detach().cpu().numpy()\n",
    "            # convert the each class probability distribution to the softmax probability distribution. meaning that the probabilities will add up to 1 between different classes\n",
    "            preds = softmax(output, axis=1)\n",
    "\n",
    "            input_images = batch['image'].detach().cpu().numpy()\n",
    "            # rearrange the tensor to the format of (batch, height, width, channel)\n",
    "            preds = rearrange(preds, 'b c h w -> b h w c')\n",
    "            input_images = rearrange(input_images, 'b c h w -> b h w c')\n",
    "            batch_mean = rearrange(batch['mean'], 'b c 1 1 -> b 1 1 c').detach().cpu().numpy()\n",
    "            batch_std = rearrange(batch['std'], 'b c 1 1 -> b 1 1 c').detach().cpu().numpy()\n",
    "\n",
    "            for b in range(output.shape[0]):# output.shape[0] is the batch size. so this code is iterating through each image in the batch\n",
    "\n",
    "                pred = preds[b]\n",
    "                metadata = batch['metadata'][b]\n",
    "                input_image = input_images[b]\n",
    "                region_name = metadata['region_name']\n",
    "\n",
    "                # Check if image stitcher exists for this region.\n",
    "                if region_name not in pred_canvases.keys():\n",
    "                    # Get base save directories.\n",
    "                    pred_save_dir = os.path.join(base_save_dir, region_name + '_pred')\n",
    "\n",
    "                    # Initialize image stitchers.\n",
    "                    pred_canvases[region_name] = ImageStitcher(pred_save_dir, save_backend='tifffile', save_ext='.tif')\n",
    "                \n",
    "                # Add input image and prediction to stitchers.\n",
    "                unnorm_img = (input_image * batch_std[b]) + batch_mean[b]\n",
    "                image_name = os.path.splitext(os.path.split(metadata['image_path'])[1])[0]\n",
    "                pred_canvases[region_name].add_image(pred, image_name, metadata['crop_params'], metadata['crop_params'].og_height, metadata['crop_params'].og_width)\n",
    "\n",
    "    # Convert stitched images to proper format.\n",
    "    for region_name in pred_canvases.keys():\n",
    "        # Combine images.\n",
    "        pred_canvas = pred_canvases[region_name].get_combined_images()\n",
    "\n",
    "        for image_name, image in pred_canvas.items():\n",
    "            # Figure out the predicted class.\n",
    "            pred = np.clip(image.argmax(axis=2), 0, 1)\n",
    "            save_path = os.path.join(pred_canvases[region_name].save_dir, image_name + '.tif')\n",
    "            print(f'Saving {save_path}')\n",
    "            Image.fromarray((pred*255).astype('uint8')).save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.2 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../Trained_model/CBAM/checkpoints/THP_CBAM_HPC.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the  1 th iteration, out of  3\n",
      "This is the  1 th iteration, out of  3\n",
      "We are infering  /Users/zhijiezhang/Current_Projects/Zhijie_PL_Pipeline/DATA/trif_example/2018_0411-3\n",
      "We are infering  /Users/zhijiezhang/Current_Projects/Zhijie_PL_Pipeline/DATA/trif_example/2018_0411-3\n",
      "check point file path:  /Users/zhijiezhang/Current_Projects/Zhijie_PL_Pipeline/Trained_model/CBAM/checkpoints/THP_CBAM_HPC.ckpt\n",
      "Saving inference to:  /Users/zhijiezhang/Current_Projects/Zhijie_PL_Pipeline/Infered_result/trif1\n",
      "2018_0411-3\n",
      "Number of images in all dataset: 1\n",
      "Model used!!!!!!!!!:  <class 'PL_Support_Codes.models.unet.UNet_CBAM'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhijiezhang/miniconda3/envs/geotorchee/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/zhijiezhang/miniconda3/envs/geotorchee/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!\n",
      "Model used:  unet_cbam\n",
      "n_classes:  3\n",
      "{'ms_image': 4}\n",
      "2\n",
      "optimizer_name:  adam\n",
      "0.0005\n",
      "!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!\n",
      "!!!!!! CUDA is not available, using MPS !!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions:   2%|\u001b[32m▏         \u001b[0m| 1/41 [00:11<07:43, 11.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe are infering \u001b[39m\u001b[38;5;124m\"\u001b[39m, full_dir_path)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe are infering \u001b[39m\u001b[38;5;124m\"\u001b[39m, full_dir_path)\n\u001b[0;32m---> 25\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 78\u001b[0m, in \u001b[0;36minfer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m pred_canvases \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;66;03m#no_grad() prevents gradiant calculation, which is not needed for inference.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# breakpoint()\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolour\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgreen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGenerating predictions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move batch to device.\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Current_Projects/Zhijie_PL_Pipeline/Zhijie_PL_Pipeline/PL_Support_Codes/datasets/batch_infer.py:437\u001b[0m, in \u001b[0;36mBatch_Infer_Dataset.__getitem__\u001b[0;34m(self, index, output_metadata)\u001b[0m\n\u001b[1;32m    434\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[index]\n\u001b[1;32m    435\u001b[0m crop_params \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrop_params\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 437\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_crop_norm_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrop_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresize_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcrop_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mog_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mog_width\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Normalize by norm_params.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m image, mean, std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(image, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor)\n",
      "File \u001b[0;32m~/Current_Projects/Zhijie_PL_Pipeline/Zhijie_PL_Pipeline/PL_Support_Codes/datasets/batch_infer.py:176\u001b[0m, in \u001b[0;36mBatch_Infer_Dataset._load_crop_norm_image\u001b[0;34m(self, image_path, crop_params, channels, resize_dims, backend)\u001b[0m\n\u001b[1;32m    172\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_crop_norm_S2_image(image_path, crop_params,\n\u001b[1;32m    173\u001b[0m                                           channels, resize_dims,\n\u001b[1;32m    174\u001b[0m                                           backend)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 176\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_crop_norm_PS_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL8\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    180\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_crop_norm_L8_image(image_path, crop_params,\n\u001b[1;32m    181\u001b[0m                                           channels, resize_dims,\n\u001b[1;32m    182\u001b[0m                                           backend)\n",
      "File \u001b[0;32m~/Current_Projects/Zhijie_PL_Pipeline/Zhijie_PL_Pipeline/PL_Support_Codes/datasets/batch_infer.py:336\u001b[0m, in \u001b[0;36mBatch_Infer_Dataset._load_crop_norm_PS_image\u001b[0;34m(self, image_path, crop_params, channels, resize_dims, backend)\u001b[0m\n\u001b[1;32m    334\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [channels, height, width]\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtifffile\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 336\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mtifffile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [height, width, channels]\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [channels, height, width]\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/site-packages/tifffile/tifffile.py:1274\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m store\n\u001b[1;32m   1273\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m zarr_selection(store, selection, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m-> 1274\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtif\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m                \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m                \u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m                \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmaxworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbuffersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffersize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m                \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, (FileHandle, BinaryIO)):\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBinaryIO not supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/site-packages/tifffile/tifffile.py:4513\u001b[0m, in \u001b[0;36mTiffFile.asarray\u001b[0;34m(self, key, series, level, squeeze, out, maxworkers, buffersize)\u001b[0m\n\u001b[1;32m   4511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m page0 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4512\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage is None\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 4513\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpage0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffersize\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4517\u001b[0m     result \u001b[38;5;241m=\u001b[39m stack_pages(\n\u001b[1;32m   4518\u001b[0m         pages, out\u001b[38;5;241m=\u001b[39mout, maxworkers\u001b[38;5;241m=\u001b[39mmaxworkers, buffersize\u001b[38;5;241m=\u001b[39mbuffersize\n\u001b[1;32m   4519\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/site-packages/tifffile/tifffile.py:9309\u001b[0m, in \u001b[0;36mTiffPage.asarray\u001b[0;34m(self, out, squeeze, lock, maxworkers, buffersize)\u001b[0m\n\u001b[1;32m   9299\u001b[0m             out[\n\u001b[1;32m   9300\u001b[0m                 s, d : d \u001b[38;5;241m+\u001b[39m shape[\u001b[38;5;241m0\u001b[39m], h : h \u001b[38;5;241m+\u001b[39m shape[\u001b[38;5;241m1\u001b[39m], w : w \u001b[38;5;241m+\u001b[39m shape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   9301\u001b[0m             ] \u001b[38;5;241m=\u001b[39m segment[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9304\u001b[0m                 : keyframe\u001b[38;5;241m.\u001b[39mimagewidth \u001b[38;5;241m-\u001b[39m w,\n\u001b[1;32m   9305\u001b[0m             ]\n\u001b[1;32m   9306\u001b[0m         \u001b[38;5;66;03m# except IndexError:\u001b[39;00m\n\u001b[1;32m   9307\u001b[0m         \u001b[38;5;66;03m#     pass  # corrupted file, for example, with too many strips\u001b[39;00m\n\u001b[0;32m-> 9309\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffersize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9314\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   9315\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fullsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   9316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   9317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[1;32m   9319\u001b[0m result\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m keyframe\u001b[38;5;241m.\u001b[39mshaped\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/site-packages/tifffile/tifffile.py:9119\u001b[0m, in \u001b[0;36mTiffPage.segments\u001b[0;34m(self, lock, maxworkers, func, sort, buffersize, _fullsize)\u001b[0m\n\u001b[1;32m   9110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(maxworkers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   9111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m segments \u001b[38;5;129;01min\u001b[39;00m fh\u001b[38;5;241m.\u001b[39mread_segments(\n\u001b[1;32m   9112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataoffsets,\n\u001b[1;32m   9113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabytecounts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9117\u001b[0m         flat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   9118\u001b[0m     ):\n\u001b[0;32m-> 9119\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m executor\u001b[38;5;241m.\u001b[39mmap(decode, segments)\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniconda3/envs/geotorchee/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "counter = 1\n",
    "# Loop through each sub-directory in the root folder\n",
    "for dir in os.listdir(ROOT_FOLDER):\n",
    "    full_dir_path = os.path.join(ROOT_FOLDER, dir)\n",
    "    if os.path.isdir(full_dir_path):\n",
    "        FOLDER_NAME = full_dir_path\n",
    "\n",
    "        # Update the JSON file\n",
    "        with open(JSON_FILE, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            data['batch_infer'] = FOLDER_NAME\n",
    "\n",
    "        with open(JSON_FILE, 'w') as json_file:\n",
    "            json.dump(data, json_file)\n",
    "\n",
    "        # Execute the command\n",
    "        print(\"This is the \", counter, \"th iteration, out of \", len(os.listdir(ROOT_FOLDER)))\n",
    "        print(\"This is the \", counter, \"th iteration, out of \", len(os.listdir(ROOT_FOLDER)))\n",
    "        print(\"We are infering \", full_dir_path)\n",
    "        print(\"We are infering \", full_dir_path)\n",
    "        infer()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geotorchee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
